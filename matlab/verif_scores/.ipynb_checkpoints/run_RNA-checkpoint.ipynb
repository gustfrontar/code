{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos las librerias y funciones que necesitaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3a1b171a907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import Modelos\n",
    "import set_dataset as ds\n",
    "import verificacion as ver\n",
    "import plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fijamos la semilla\n",
    "\n",
    "Esto es importante para lograr la reproducibilidad de los experimentos que realicemos, y que no sea un factor de incertidumbre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modelos.define_seed(seed=1029)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las variables que utilizaremos como entrada (Input) y objetivo (Target) en la red neuronal que utilicemos\n",
    "\n",
    "Las variable de Input es ctt (Temperatura de tope de nube, [K])\n",
    "\n",
    "Las variable de Target es rainrate (Tasa de lluvia, [mm/h]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_name =  \"ctt\"\n",
    "Target_name = \"rainrate\"\n",
    "Experimento = Input_name+\" vs \"+ Target_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparametros\n",
    "\n",
    "Los hiperparametros son definidos, generalmente de forma manual(*), y que determina el entrenamiento de los modelos. En cambio un parametro, es estimado en el entrenamiento.\n",
    "\n",
    "(*) Existen algoritmos que buscan la optimización de los hiperparametros en un proceso conocido como \"selección de modelos\". En este curso no entraremos en detalles sobre este punto, pero es importante que conozcan que existe, entre los más usuales se encuentran:\n",
    "\n",
    "Grid Search - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "Random Search - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "HyperOpt - https://hyperopt.github.io/hyperopt/\n",
    "Su paper: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.704.3494&rep=rep1&type=pdf\n",
    "\n",
    "Auto Machine Learning (AutoML) - https://www.automl.org/automl/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numero_exp = 3 #Una forma de identificar la configuración que utilizamos\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#Hiperparametros\n",
    "batch_size= 100\n",
    "max_epochs = 50\n",
    "learning_rate = 1e-3 ; milestones = [20,40] ; gamma=0.5\n",
    "weight_decay = 1e-5\n",
    "\n",
    "#Porcentaje de reparticion de los conjuntos de Train / Validation / Testing\n",
    "train_ratio = .8\n",
    "validation_ratio = .1\n",
    "test_ratio = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Es un recurso para aumentar de manera artificial los datos con los cuales entrenamos la red neuronal. Existen diferentes metodologías para llevarlo a cabo, por ejemplo a la imagen original rotarla o añadirle un ruido, que altera suavemente a la imagen, generando una imagen nueva, pero que conserve sus características. En este caso, se deja la opción realizar un data augmentation con rotación alrededor del eje horizontal, vertical y de ambos, de esta forma por cada imagen perteneciente al entrenamiento, se generaran 3 imagenes adicionales, lo cual cuadriplica el tamaño del dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Aug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los datos y DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = ds.get_data(\"../DATA/datos_WRF_300.npz\", Input_name, Target_name) #Levantamos los datos\n",
    "nx, ny = Data[\"nx\"], Data[\"ny\"] #Obtenemos las dimensiones de las imagenes\n",
    "dataset = ds.set_up_data(Data) #Esta función permite permite dejar los datos listos para ser utilizados por el DataLoader\n",
    "\n",
    "#Mostramos por pantalla el rango de los datos de Input y Target\n",
    "print(dataset.xmin,dataset.xmax,dataset.ymin,dataset.ymax)\n",
    "\n",
    "indices = range(dataset.y_data.shape[2]) #Con el largo del dataset cuento cuantos hay y genero un vector de indices\n",
    "\n",
    "train_ids, rest_ids = train_test_split(indices, test_size=1 - train_ratio)\n",
    "val_ids, test_ids = train_test_split(rest_ids, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "if Data_Aug:\n",
    "    #Genero el dataset de entrenamiento con data augmentation\n",
    "    train_aug_dataset = ds.augment_data(dataset.x_data[:,:,train_ids],dataset.y_data[:,:,train_ids])\n",
    "    train_aug_dataset = ds.set_up_data(train_aug_dataset) #Enchufo el dataset con augmentation\n",
    "    trainloader = DataLoader(train_aug_dataset, batch_size=batch_size)          \n",
    "else:\n",
    "    #Dataset de entrenamiento sin data augmentation\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    trainloader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    \n",
    "test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "testloader = DataLoader(dataset, batch_size=batch_size, sampler=test_subsampler)\n",
    "        \n",
    "val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "valloader = DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "\n",
    "dataloaders = {\n",
    "            'train' : trainloader,\n",
    "            'valid' : valloader,\n",
    "            'test' : testloader\n",
    "            }\n",
    "\n",
    "print(\"Muestras de Train/Valid/Test: \",(len(train_ids),len(val_ids),len(test_ids)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos el modelo a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNA(nn.Module):\n",
    "    def __init__(self, nx, ny):\n",
    "        super().__init__()\n",
    "        #Con el _init_ definimos los atributos de la clase que estamos definiendo\n",
    "        #El super() nos permite utilizar la clase y atributos a partir de usar explicitamente su nombre\n",
    "        self.nx, self.ny, self.dim = nx, ny, nx*ny\n",
    "        self.Linear_1 = nn.Linear(int(self.dim), int(self.dim/2), bias=True)\n",
    "        self.Linear_2 = nn.Linear(int(self.dim/2), int(self.dim), bias=True)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        x = x.view(batch,self.nx*self.ny)\n",
    "        x= self.Linear_1(x)\n",
    "        x = self.activation(x)\n",
    "        x= self.Linear_2(x)\n",
    "        x = x.view(batch,self.nx,self.ny)\n",
    "        return self.activation(x)\n",
    "\n",
    "model = RNA(nx = nx, ny = ny)\n",
    "model.to(device) #De existir GPU se computaría por allí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path donde guardaremos las salidas del modelo\n",
    "Directorio_out = \"../salidas_RNA\"+\"/\"+Experimento+\" \"+str(Numero_exp)+\"/\" #Ver como crear directorio\n",
    " \n",
    "if not os.path.exists(Directorio_out):\n",
    "    # Creao un nuevo directorio si no existe (para guardar las imagenes)\n",
    "    os.makedirs(Directorio_out)\n",
    "    print(\"El nuevo directorio asociado a \"+ Experimento +\" \"+str(Numero_exp)+\" ha sido creado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras configuraciones: Función de costo, Optimizador e Inicialización de pesos.\n",
    "\n",
    "Estrictamente lo siguiente que definiremos son hiperparametros también. Ya que son configuraciones que definimos en la etapa de pre-entrenamiento, y son sumamente importantes porque configuran el objetivo y forma de entrenamiento. \n",
    "\n",
    "A partir del algoritmo de **_Backprogation_** se propagara el costo(loss) y se imputará sobre los parámetros (pesos y bias) de la red neuronal.\n",
    "\n",
    "El **_Optimizador_** es el método de actualización de los pesos en cada época.\n",
    "\n",
    "Los pesos se actualizaran siguiendo la idea del algoritmo de descenso del gradiente, donde la función a minimizar es la **_Función de costo (Loss Function)_** que definimos.\n",
    "\n",
    "El \"punto de partida\" de los parametros a optimizar para la minimización de la Función de costo, está dada por la **_distribución inicial de los pesos_**. La cual es importante considerar debido a que puede colaborar a lograr una rápida convergencia de la red neuronal hacia el mínimo global de la función de costo (de encontrarse) o por el contrario atentar contra esto y requerir un mayor número de eṕocas para lograr la convergencia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Definimos la función de costo que queremos minimizar, y también el método de calculo sobre el batch.\n",
    "MSE_Loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "#Definimos el optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) \n",
    "\n",
    "Modelos._initialize_weights(model) #Definimos los pesos iniciales con los cuales inicia el modelo antes de entrenarlo\n",
    "\n",
    "#Definimos el Scheduler para el Learning Rate\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma = gamma, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listas donde guardamos las metricas y la loss para cada conjunto\n",
    "RMSE, BIAS, Corr_P, Corr_S = [], [], [], []\n",
    "loss_train, loss_val, loss_test = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la Red Neuronal Artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print('Epoca: %d ' % (epoch))\n",
    "            \n",
    "    for phase in ['train', 'valid', 'test']:\n",
    "        print(phase)\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        batch_counter = 0\n",
    "\n",
    "        # Iterate over data\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            if phase == 'train':\n",
    "                outputs = model(inputs)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "            loss = MSE_Loss(outputs.float(), labels.float())\n",
    "                    \n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if phase == 'test':\n",
    "                if batch_counter == 0:\n",
    "                    target_test = ds.denorm(labels.float(),dataset.ymin,dataset.ymax)\n",
    "                    modelo_test = ds.denorm(outputs,dataset.ymin,dataset.ymax)\n",
    "                else:\n",
    "                    target_test = np.append(target_test,ds.denorm(labels.float(),dataset.ymin,dataset.ymax),axis=0)\n",
    "                    modelo_test = np.append(modelo_test,ds.denorm(outputs,dataset.ymin,dataset.ymax),axis=0)\n",
    "                \n",
    "                if epoch == max_epochs-1:\n",
    "                    if batch_counter == 0:\n",
    "                        input_test = ds.denorm(inputs,dataset.xmin,dataset.xmax)\n",
    "                    else:\n",
    "                        input_test = np.append(input_test, ds.denorm(inputs,dataset.xmin,dataset.xmax), axis = 0)\n",
    "            batch_counter += 1\n",
    "            \n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "\n",
    "        #Calculo de la loss de la epoca\n",
    "        if phase == 'train':\n",
    "            loss_train_epoch = running_loss/batch_counter\n",
    "            loss_train.append(loss_train_epoch)\n",
    "            print('Loss: %f' %(loss_train_epoch))\n",
    "        if phase == 'valid':\n",
    "            loss_val_epoch = running_loss/batch_counter\n",
    "            loss_val.append(loss_val_epoch)\n",
    "            print('Loss: %f' %(loss_val_epoch))\n",
    "        if phase == 'test':\n",
    "            loss_test_epoch = running_loss/batch_counter\n",
    "            loss_test.append(loss_test_epoch)\n",
    "            print('Loss: %f' %(loss_test_epoch))\n",
    "            \n",
    "###################################\n",
    "    #Calculo de metricas RMSE, BIAS, Correlacion de Pearson y Spearman\n",
    "    metricas = [ver.rmse(modelo_test,target_test),\n",
    "                ver.bias(modelo_test,target_test),\n",
    "                ver.corr_P(modelo_test,target_test),\n",
    "                ver.corr_S(modelo_test,target_test)]\n",
    "\n",
    "    #Guardo la loss para cada Fold y Epoca        \n",
    "    RMSE.append(metricas[0]) ; BIAS.append(metricas[1]), Corr_P.append(metricas[2]), Corr_S.append(metricas[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploteamos y evaluamos el conjunto de Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Muestras = random.sample(range(len(test_ids)),10)\n",
    "plots.plotting(Directorio_out, input_test, target_test, modelo_test,\n",
    "                loss_train, loss_val, loss_test, \n",
    "                RMSE,BIAS,Corr_P, Corr_S,\n",
    "                Experimento, Input_name, Target_name,\n",
    "                max_epochs, Muestras, nx ,ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando el modelo, y los datos de Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model.load_state_dict(torch.load(Directorio_out+\"Modelo_exp_\"+str(Numero_exp)+\".pth\"))\n",
    "\n",
    "\n",
    "if True: #Guardar los datos de test\n",
    "    np.savez(Directorio_out+\"Datos_test_\"+Input_name+\"_vs_\"+Target_name+\"_\"+str(Numero_exp)+\".npz\",\n",
    "         Input=input_test.reshape(input_test.shape[0],nx,ny),\n",
    "         Target=target_test.reshape(target_test.shape[0],nx,ny),\n",
    "         Modelo=modelo_test.reshape(modelo_test.shape[0],nx,ny),\n",
    "         loss_train = loss_train, loss_val = loss_val, loss_test = loss_test,\n",
    "         RMSE = RMSE, BIAS = BIAS, Corr_P = Corr_P, Corr_S = Corr_S,\n",
    "         Experimento = Experimento, Input_name = Input_name, Target_name = Target_name,\n",
    "         max_epochs = max_epochs, nx = nx, ny = ny)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
